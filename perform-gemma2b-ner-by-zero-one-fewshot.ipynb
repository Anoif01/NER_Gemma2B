{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7837171,"sourceType":"datasetVersion","datasetId":4594016},{"sourceId":7871711,"sourceType":"datasetVersion","datasetId":4607176},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìù Exploring Gemma_2b_en in NER Tasks\n\nüö©In this Notebook, I aim to showcase and explore the performance of <span style='color: brown;'><b>gemma_2b_en model</b></span> in NER (Named Entity Recognition) tasks. The learning paradigms utilized in this Notebook are <span style='color: brown;'><b>Zero Shot Learning</b></span>, <span style='color: brown;'><b>One Shot Learning</b></span>, <span style='color: brown;'><b>Few Shot Learning</b></span> (contains 3, 5, 7, 9 Shots). \n\n\nüìÉThe Input-Dataset <span style='color: orange;'><b>\"pii-combined-data\"</b></span> is from [The Learning Agency Lab - PII Data Detection Competition](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/data) as well as additional external data [here](https://www.kaggle.com/code/valentinwerner/fix-punctuation-tokenization-external-dataset). The competition dataset contains about 22,000 essays from students in an online course. Task is to <span style='color: brown;'><b>find and label personally identifiable information (PII)</b></span> in these essays.\n\nüìÉ The Input-Dataset <span style='color: orange;'><b>\"temp-test-gemma-output\"</b></span> is utilized to prevent re-running the learning phase because executing all learning paradigms would be time-consuming (more than 3 hours). With this dataset, I've saved all the outputs in a dictionary, allowing us to save learning time and conduct further analysis.\n\n\n<blockquote>    \n<p style=\"color:green; font-weight:bold;\">PII Types: </p>\n    \n<ol>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">NAME_STUDENT</u>: Student names (excluding instructors, authors, etc.).\n</li>     \n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">EMAIL</u>: Student email addresses.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">USERNAME</u>: Student usernames on any platform.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">ID_NUM</u>: Student identification numbers (e.g., student ID, social security number).\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">PHONE_NUM</u>: Student phone numbers.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">URL_PERSONAL</u>: URLs that could identify a student.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">STREET_ADDRESS</u>: Student street addresses (e.g., home address).\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">STREET_ADDRESS</u>: Student street addresses (e.g., home address).\n</li>\n</ol>\n</blockquote> \n    \n    \n##  <span style='color: Orange;'><b>Overall Sections</b></span>\n\n#### 1. **Load and Analyze Data** üìä\n\n<blockquote>\nüí°In the first part, we will complete the following steps: <u style=\"color: brown; font-style: italic;\">loading</u>, <u style=\"color: brown; font-style: italic;\">preprocessing</u>, <u style=\"color: brown; font-style: italic;\">splitting</u> data into training and testing datasets, and <u style=\"color: brown; font-style: italic;\">visualizing label distribution</u> using Plotly.\n</blockquote>\n\n#### 2. **Prepare Prompt for Gemma** üé®\n<blockquote>\n<p>\nüí°In the second part, we will prepare the <u style=\"color: brown; font-style: italic;\">Answer dictionary</u> for each exampleÔºåand then <u style=\"color: brown; font-style: italic;\">generate prompts</u> for different learning paradigms. \n</p>\n    \n<p style=\"font-weight:bold;\">Strategies: </p>\n    \n<ol>\n    <li>\n        For <u style=\"color: brown; font-weight: bold;\">Zero Shot Learning</u>, only include context information to inform the model of the task.\n    </li>\n    <li>\n        For <u style=\"color: brown; font-weight: bold;\">other learning paradigms</u>, include context information along with N examples.\n    </li>\n</ol>\n\n<p style=\"font-weight:bold;\">Here is an example of Answer dictionary (Synthetic Data): </p>\n    \n{<span style='font-size: 12px; color: green;'><b>'EMAIL'</b></span>: ['scottsherman@yahoo.com', 'larsenjoseph@gmail.com'],\n\n <span style='font-size: 12px; color: green;'><b>'ID_NUM'</b></span>: ['AGX-6811'],\n\n <span style='font-size: 12px; color: green;'><b>'NAME_STUDENT'</b></span>: ['Stephenxxx Morgan'],\n\n <span style='font-size: 12px; color: green;'><b>'PHONE_NUM'</b></span>: ['(485)728-5578', '(261)318-0141'],\n\n <span style='font-size: 12px; color: green;'><b>'STREET_ADDRESS'</b></span>: ['70 Harrison Manor Suite 01, Franklinville, VT 78297'],\n\n <span style='font-size: 12px; color: green;'><b>'URL_PERSONAL'</b></span>: ['https://github.com/123'],\n\n <span style='font-size: 12px; color: green;'><b>'USERNAME'</b></span>: ['nathanby']}\n\n    \n</blockquote>\n\n#### 3. **Load Gemma Model** ü§ñ\n<blockquote>\nüí°In the third part, we will import the Gemma_2b_en model from Kaggle and set <u style=\"color: brown; font-style: italic;\">TopKSampler</u> with a seed for text generation.\n</blockquote>\n\n#### 4. **N Shot Learning Phase** üìö\n\n#### 5. **Post-process of model's outputs** üõ†Ô∏è\n<blockquote>\nüí°In the last part, we will post-process the model's output results by <u style=\"color: brown; font-style: italic;\">extracting the real answers</u> and then convert them into a usable <u style=\"color: brown; font-style: italic;\">dictionary format</u> to calculate the performance: <u style=\"color: brown; font-style: italic;\">F5 score</u>. (This score is used by the competition, means \"Recall being 5 times more important than Precision\".) \n</blockquote>\n\n#### **References** üìñ\n<blockquote>\n    \n[piidd-let-s-go-higher](https://www.kaggle.com/code/hyunsoolee1010/piidd-let-s-go-higher)\n    \n[rule-based-approach](https://www.kaggle.com/code/emiz6413/rule-based-approach)\n    \n[kaggle-qa-with-gemma-kerasnlp-starter](https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter)\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"#  üîß<span style='color: Orange;'><b>0. Import necessary libraries, Set-up Configs and Seed</b></span>","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use \"jax\" tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n\nimport time\nimport random\nimport json\nimport numpy as np\nimport pandas as pd\n\n# pandas progress bar and learning progress bar\nfrom tqdm.notebook import tqdm\ntqdm.pandas() \n\n# We will use keras_nlp for Gemma model\nimport keras\nimport keras_nlp\n\n# We will use spacy for tokenizer and re for Regex during post-process of predictions\nfrom spacy.lang.en import English\nimport re\n\n# Import the libiaries for visualisation\nfrom IPython.display import display, Markdown\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T11:58:24.839608Z","iopub.execute_input":"2024-03-19T11:58:24.840375Z","iopub.status.idle":"2024-03-19T11:58:51.536202Z","shell.execute_reply.started":"2024-03-19T11:58:24.840332Z","shell.execute_reply":"2024-03-19T11:58:51.534959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    keras.utils.set_random_seed(seed)\n\n# Set seed for reproducibility   \nseed = 42 \nset_seed(seed)\n\n# Learning Configs\nmodel_name = \"gemma_2b_en\" # name of pretrained Gemma\ntrain_path = \"/kaggle/input/pii-combined-data/train.json\"\ntest_path = \"/kaggle/input/pii-combined-data/moredata_dataset_fixed.json\"\n\n# Set the maximum prediction tokens length\nmax_length_pred_zeroshot = 1024*1\nmax_length_pred_oneshot = 1024*2\nmax_length_pred_3shot = 1024*4\nmax_length_pred_5shot = 1024*6\nmax_length_pred_7shot = 1024*7\nmax_length_pred_9shot = 1024*8","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:58:54.206686Z","iopub.execute_input":"2024-03-19T11:58:54.208598Z","iopub.status.idle":"2024-03-19T11:58:54.218642Z","shell.execute_reply.started":"2024-03-19T11:58:54.208533Z","shell.execute_reply":"2024-03-19T11:58:54.217377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  üìà<span style='color: Orange;'><b>1. Load and Quick Analyse Data</b></span>\n<blockquote>    \n<p style=\"color:brown; font-weight:bold;\">Introduction to the dataset: </p>\nWe have 5 colomns in train.json and in moredata_dataset_fixed.json. \n    \nHere I use the data include labels as test data (but not test.json) so that I can calculate the performance.\n    \n<ol>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">document (int)</u>: an integer ID of the essay.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">full_text (string)</u>: a UTF-8 representation of the essay.\n</li>     \n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">tokens (list of string)</u>: list of string representations of each token.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">trailing_whitespace (list of boolean)</u>: list of boolean values indicating whether each token is followed by whitespace.\n</li>\n<li>\n<u style=\"color: green; font-weight: bold;font-size: 12px\">labels (list of string)</u>: list of token labels in BIO format.\n</li>\n</ol>\n</blockquote> \n","metadata":{}},{"cell_type":"code","source":"# Check positive samples (contain relevant labels) and downsample negative samples (contain only \"OTHER\")\n# Downsample to fix dataset imbalance.\ndef get_pos_neg(data):\n    pos=[] # positive samples\n    neg=[] # negative samples\n    for d in data:\n        if any(np.array(d[\"labels\"]) != \"O\"): \n            pos.append(d)\n        else: \n            neg.append(d)\n    # downsample the negative examples\n    if len(neg)>len(pos):\n        neg = neg[:len(pos)//2]\n        print(\"Downsampled the negative examples.\")\n    return pos,neg\n\ndef transform_to_pd(json_data):\n    return pd.DataFrame({\n                        \"full_text\": [x[\"full_text\"] for x in json_data],\n                        \"document\": [str(x[\"document\"]) for x in json_data],\n                        \"tokens\": [x[\"tokens\"] for x in json_data],\n                        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in json_data],\n                        \"labels\": [x[\"labels\"] for x in json_data],})\n\n# Load train, test json data\ndata1 = json.load(open(train_path))\ndata2 = json.load(open(test_path))\n\np1, n1 = get_pos_neg(data1)\np2, n2 = get_pos_neg(data2)\n\n# The number of test set > the number of training set, transfer 95% the test data to the training set.\npercent= 0.95\nlength = int(len(p2)*percent)\np2_train = p2[:length]\np2_test = p2[length:]\n\n# Make train dataframe and test dataframe\ntrain_data = transform_to_pd(p1+n1+p2_train)\ntest_data = transform_to_pd(p2_test+n2)\n\n# ================Logs================\nprint(\"Original Train datapoints: \", len(data1))\nprint(\"Train datapoints: \", len(train_data))\nprint(\"Train Positive datapoints: \", len(p1)+len(p2_train))\nprint(\"Train Negative datapoints: \", len(n1))\nprint(\"\\n\")\n\nprint(\"Original Test datapoints: \", len(data2))\nprint(\"Test datapoints: \", len(test_data))\nprint(\"Test Positive datapoints: \", len(p2_test))\nprint(\"Test Negative datapoints: \", len(n2))\nprint(\"\\n\")\n\ndisplay(train_data.head(3))\nprint('\\n')\ndisplay(test_data.head(3))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:58:56.669661Z","iopub.execute_input":"2024-03-19T11:58:56.670173Z","iopub.status.idle":"2024-03-19T11:59:04.029316Z","shell.execute_reply.started":"2024-03-19T11:58:56.670136Z","shell.execute_reply":"2024-03-19T11:59:04.028410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  üìà<span style='color: Orange;'><b>1.1 Pre-process DataFrames</b></span>\n\nüëÅÔ∏è‚ÄçIn the dataframe, I noticed that the <span style=\"color:brown; font-weight:bold;\">\"labels\" column follows the BIO labeling scheme</span>. (List of BIO labels for each token.) \n\n<blockquote> \nSpecifically, <span style=\"color:green; font-weight:bold;\">\"B-\"</span> denotes the beginning of a label, and the remaining tokens are marked with <span style=\"color:green; font-weight:bold;\">\"I-\"</span>. \n</blockquote> \n\nüí°However, this kind of \"labels\" doesn't provide a clear understanding of how many labels are present in each example or what each label represents. Therefore, in this section, I utilized <span style=\"color:brown; font-weight:bold;\">One-Hot Encoding</span> and some <span style=\"color:brown; font-weight:bold;\">Text Processing</span> to add individual columns for each label name in the dataframe. \n\n<blockquote>\nThe values in these One-Hot columns are either 0 or 1, with 0 indicating the absence of the label and 1 indicating its presence.\n</blockquote> ","metadata":{}},{"cell_type":"code","source":"# Let's check out the multi-labels' presences in each example (One-Hot encoding labels)\ndef get_label_names(x):\n    labels = set()\n    for ele in x:\n        if ele != \"O\":\n            # element is like: B-STREET_ADDRESS\n            # remove \"B-\" or \"I-\"\n            e = ele.split('-')[-1]\n            labels.add(e)\n    return list(labels)\n\ndef onehot_labels(df, str_):\n    # get numbers of tokens\n    df[\"token_nums\"] = df[\"tokens\"].apply(lambda x: len(x))\n    \n    # get all and unique labels and total count of unique labels of each example\n    df[\"unique_labels\"] = df[\"labels\"].apply(lambda x: get_label_names(x))\n    df[\"unique_labels_count\"] = df[\"unique_labels\"].apply(lambda x: len(x))\n\n    # get complete labels in dataset\n    labels = list(set(element for sublist in df[\"unique_labels\"] for element in sublist)) + [\"OTHER\"]\n    print(f\"Show all {len(labels)} labels in the {str_} dataset:\\n {labels}\")\n    print('\\n')\n\n    # One-Hot encoding for each label\n    df[labels] = 0\n    for index, data_ in df.iterrows():\n        if data_.unique_labels_count != 0:\n            for label in data_.unique_labels:\n                df.at[index, label] = 1\n        else:\n            df.at[index, \"OTHER\"] = 1\n    return df, labels\n\ntrain_data, train_labels = onehot_labels(train_data, \"train\")\ntest_data, test_labels = onehot_labels(test_data, \"test\")\n\nsum_train = train_data[train_labels].sum(axis=0)\nsum_test = test_data[test_labels].sum(axis=0)\n\n# ================Logs================\ndisplay(train_data.head(3))\nprint('\\n')\ndisplay(test_data.head(3))\nprint('\\n')\n\nprint(\"Train dataset labels distribution: \\n\", sum_train)\nprint('\\n')\nprint(\"Test dataset labels distribution: \\n\", sum_test)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:04.030897Z","iopub.execute_input":"2024-03-19T11:59:04.031725Z","iopub.status.idle":"2024-03-19T11:59:05.542809Z","shell.execute_reply.started":"2024-03-19T11:59:04.031687Z","shell.execute_reply":"2024-03-19T11:59:05.541572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  üìà<span style='color: Orange;'><b>1.2 PlotlyÔºöLabels Distribution in Train/Test Data</b></span>\n","metadata":{}},{"cell_type":"code","source":"# Plotly\nfont_family = \"Arial\"\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]]) #, subplot_titles=(\"TrainSet\", \"TestSet\")\n\n# Define data to plot\nlabels1 = train_labels\nvalues1 = sum_train\n\nlabels2 = test_labels\nvalues2 = sum_test\n\n# Add the first pie chart\nfig.add_trace(go.Pie(labels=labels1, values=values1, name=\"Distribution of labels in TrainSet\", textinfo='label+percent'),\n              1, 1)\n\n# Add the second pie chart\nfig.add_trace(go.Pie(labels=labels2, values=values2, name=\"Distribution of labels in TestSet\", textinfo='label+percent'),\n              1, 2)\n\n# Update Layout Settings\nfig.update_layout(width=1000, height=700, margin=dict(l=75,r=50,b=50,t=50,),paper_bgcolor=\"WHITE\")\n\nfig.update_layout(title_text='Distribution of labels in TrainSet / TestSet',\n                  font=dict(family=font_family,\n                            size=10,\n                            color=\"#75767A\"),\n                 )\nfig.update_layout(title_font_family=font_family,\n                  title_font_color='BLACK',\n                  title_font_size=18,\n                  title_x=0.5)\n\nfig.update_layout(legend_title=\"\",\n                  showlegend=True,\n                  legend=dict(x=0, y=1, orientation=\"h\")\n                 )\n\nfig.add_annotation(text=\"TrainSet\", x=0.18, y=0.05, xref=\"paper\", yref=\"paper\",\n                   showarrow=False, font=dict(size=16))\nfig.add_annotation(text=\"TestSet\", x=0.82, y=0.05, xref=\"paper\", yref=\"paper\",\n                   showarrow=False, font=dict(size=16))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:05.544459Z","iopub.execute_input":"2024-03-19T11:59:05.544876Z","iopub.status.idle":"2024-03-19T11:59:06.309847Z","shell.execute_reply.started":"2024-03-19T11:59:05.544805Z","shell.execute_reply":"2024-03-19T11:59:06.308548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  ‚úèÔ∏è<span style='color: Orange;'><b>2. Prepare Learning Prompts for Gemma</b></span>\n","metadata":{}},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.1. Prepare Answer Dictionary</b></span>\n\nAfter adding the One-Hot columns, we also need to create a <span style='color: brown;'><b>value list for each label</b></span> to obtain the corresponding string. After obtaining the value lists, we can create an <span style='color: brown;'><b>\"answers\" colomn</b></span> to combine all the labels and their values into a dictionary.","metadata":{}},{"cell_type":"code","source":"# Get the string/values for each label\ndef value_of_labels(df, label_name):\n    new_col = label_name+'_VALUE'\n    df[new_col] = 'Nan'\n    for row_id, row in df.iterrows():\n        s = None\n        true_ = []\n        for token, label, space in zip(row[\"tokens\"], row[\"labels\"], row[\"trailing_whitespace\"]):\n            if label == \"B-\"+label_name:\n                if s is not None:\n                    if s.strip() not in true_:\n                        true_.append(s.strip())\n                space = \" \" if space else \"\"\n                s = token + space\n            if label == \"I-\"+label_name:\n                space = \" \" if space else \"\"\n                s += token + space\n        if s is not None:\n            if s.strip() not in true_:\n                true_.append(s.strip())\n        df.at[row_id, new_col] = true_\n    return df\n\n# Get the list of values of each label\nfor label_name in train_labels[:-1]: #exclude OTHER\n    value_of_labels(train_data, label_name)\n    value_of_labels(test_data, label_name)\n\ntrain_data['answers'] = train_data.apply(lambda row: {\"EMAIL\": row['EMAIL_VALUE'],\n                                                        \"ID_NUM\": row['ID_NUM_VALUE'],\n                                                        \"NAME_STUDENT\": row['NAME_STUDENT_VALUE'],\n                                                        \"PHONE_NUM\": row['PHONE_NUM_VALUE'],\n                                                        \"STREET_ADDRESS\": row['STREET_ADDRESS_VALUE'],\n                                                        \"URL_PERSONAL\": row['URL_PERSONAL_VALUE'],\n                                                        \"USERNAME\": row['USERNAME_VALUE']}, axis=1)\ntest_data['answers'] = test_data.apply(lambda row: {\"EMAIL\": row['EMAIL_VALUE'],\n                                                        \"ID_NUM\": row['ID_NUM_VALUE'],\n                                                        \"NAME_STUDENT\": row['NAME_STUDENT_VALUE'],\n                                                        \"PHONE_NUM\": row['PHONE_NUM_VALUE'],\n                                                        \"STREET_ADDRESS\": row['STREET_ADDRESS_VALUE'],\n                                                        \"URL_PERSONAL\": row['URL_PERSONAL_VALUE'],\n                                                        \"USERNAME\": row['USERNAME_VALUE']}, axis=1)\n\n# ================Logs================\ndisplay(train_data.head(3))\ndisplay(test_data.head(3))\n\nprint(f\"Show an example of train answer:\\n {train_data.answers[0]} \")\nprint('\\n')\nprint(f\"Show an example of test answer:\\n {test_data.answers[0]} \")\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:06.312532Z","iopub.execute_input":"2024-03-19T11:59:06.313782Z","iopub.status.idle":"2024-03-19T11:59:15.593280Z","shell.execute_reply.started":"2024-03-19T11:59:06.313740Z","shell.execute_reply":"2024-03-19T11:59:15.591879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.2. Prompt for Zero-Shot Learning</b></span>\n\n<span style='color: green;'><b>Zero-Shot Learning: Only providing context to define specific tasks, </b></span> without providing any examples beyond that.\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the zeroshot_prompt</b></span>: \n<blockquote>\n    Context + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Generate zero-shot example's prompt\nzeroshot_prompt = \"\\n\\nContext:\\n{Context}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\ncontext_zeroshot = f\"\"\"Please identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), \nphone number(PHONE_NUM), street address(STREET_ADDRESS), personal url(URL_PERSONAL) and user name(USERNAME). List each category and its entities in json format.\n{{'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}}\"\"\"\n\n# Generate test example's prompts\ntest_data['zeroshot_prompts'] = test_data.apply(lambda row: zeroshot_prompt.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)\n\n# Check prompts\ndef colorize_test_text(text):\n    for word, color in zip([\"Context\",\"Text\",\"Answer\"], \n                           [\"blue\",\"red\",\"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n    return text\n\ndef display_test_prompt(sample):\n    # Give colors to Text and Answer\n    sample = colorize_test_text(sample)\n    # Show sample in markdown\n    display(Markdown(sample))\n\nfor i in range(1):\n    sample = test_data['zeroshot_prompts'].tolist()[i]\n    display_test_prompt(sample)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.595283Z","iopub.execute_input":"2024-03-19T11:59:15.595666Z","iopub.status.idle":"2024-03-19T11:59:15.616703Z","shell.execute_reply.started":"2024-03-19T11:59:15.595633Z","shell.execute_reply":"2024-03-19T11:59:15.615263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.3. Prompt for One-Shot Learning</b></span>\n\n<span style='color: green;'><b>One-Shot Learning: Providing context to define specific tasks, and one example (include Text + Answer).</b></span>\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the oneshot_prompt</b></span>: \n<blockquote>\n    Context + Example1(Text+Answer) + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Gather 1 exampleÔºö\n# Here I have chosen row indexe=2501, which contains all the labels and has the fewest tokens.\n# train_data[train_data['unique_labels_count']==7].sort_values(by=['token_nums'])\nrow_oneshot = [2501] \ndf_examples_oneshot = train_data.loc[row_oneshot]\n\n# Generate example's prompt\ntrain_template_prompt = \"\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\n# Generate example's prompts\ndf_examples_oneshot['prompts'] = df_examples_oneshot.apply(lambda row: train_template_prompt.format(\n                                                                    Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]), \n                                                                    Answer=row[\"answers\"]), axis=1)\n\n# Check prompts\ndef colorize_text(text):\n    for word, color in zip([\"Text\", \"Answer\"], [\"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n    return text\n\ndef display_prompt(sample):\n    # Give colors to Text and Answer\n    sample = colorize_text(sample)\n    # Show sample in markdown\n    display(Markdown(sample))\n\nfor i in range(1):\n    sample = df_examples_oneshot['prompts'].tolist()[i]\n    display_prompt(sample)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.618584Z","iopub.execute_input":"2024-03-19T11:59:15.618988Z","iopub.status.idle":"2024-03-19T11:59:15.639105Z","shell.execute_reply.started":"2024-03-19T11:59:15.618954Z","shell.execute_reply":"2024-03-19T11:59:15.637655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate one-shot test's prompt\noneshot_prompt = \"\\n\\nContext:\\n{Context}\\n\\nExample1:\\n{Example1}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\nexample_prompts_oneshot = df_examples_oneshot['prompts'].tolist()\n\n# Generate test example's prompts\ntest_data['oneshot_prompts'] = test_data.apply(lambda row: oneshot_prompt.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Example1=example_prompts_oneshot[0], \n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)\n\n# Check prompts\ndef colorize_test_text_oneshot(text):\n    for word, color in zip([\"Context\",\"Example1\",\"Text\",\"Answer\"], \n                           [\"blue\",\"orange\",\"red\",\"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n    return text\n\ndef display_test_prompt_oneshot(sample):\n    # Give colors to Text and Answer\n    sample = colorize_test_text_oneshot(sample)\n    # Show sample in markdown\n    display(Markdown(sample))\n\nfor i in range(1):\n    sample = test_data['oneshot_prompts'].tolist()[i]\n    display_test_prompt_oneshot(sample)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.643242Z","iopub.execute_input":"2024-03-19T11:59:15.643684Z","iopub.status.idle":"2024-03-19T11:59:15.665783Z","shell.execute_reply.started":"2024-03-19T11:59:15.643633Z","shell.execute_reply":"2024-03-19T11:59:15.664515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.4. Prompt for Three-Shot Learning</b></span>\n\n<span style='color: green;'><b>Three-Shot Learning: Providing context to define specific tasks, and three examples (include Text + Answer).</b></span>\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the threeshot_prompt</b></span>: \n<blockquote>\n    Context + Example1(Text+Answer) + Example2(Text+Answer) + Example3(Text+Answer) + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Gather 3 exampleÔºö\n# You can choose the row indexe as you like\n# Here I have chosen row indexe=1319, which contains 0 label and has the fewest tokens.\n#                    row indexe=2049, which contains 4 labels and has the fewest tokens.\n#                    row indexe=2501, which contains 7 labels and has the fewest tokens.\n\nrow_3shot = [1319,2049,2501] \ndf_examples_3shot = train_data.loc[row_3shot]\n\n# Generate example's prompt\ntrain_template_prompt = \"\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\n# Generate example's prompts\ndf_examples_3shot['prompts'] = df_examples_3shot.apply(lambda row: train_template_prompt.format(\n                                                                    Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]), \n                                                                    Answer=row[\"answers\"]), axis=1)\n\n# Generate three-shot test's prompt\nprompt_3shot = \"\\n\\nContext:\\n{Context}\\n\\nExample1:\\n{Example1}\\n\\nExample2:\\n{Example2}\\n\\nExample3:\\n{Example3}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\ncontext = f\"\"\"Please identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), \nphone number(PHONE_NUM), street address(STREET_ADDRESS), personal url(URL_PERSONAL) and user name(USERNAME). List each category and its entities in json format.\"\"\"\n\nexample_prompts_3shot = df_examples_3shot['prompts'].tolist()\n\n# Generate test example's prompts\ntest_data['prompt_3shot'] = test_data.apply(lambda row: prompt_3shot.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Example1=example_prompts_3shot[0], \n                                                                            Example2=example_prompts_3shot[1], \n                                                                            Example3=example_prompts_3shot[2], \n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.667508Z","iopub.execute_input":"2024-03-19T11:59:15.667971Z","iopub.status.idle":"2024-03-19T11:59:15.683575Z","shell.execute_reply.started":"2024-03-19T11:59:15.667922Z","shell.execute_reply":"2024-03-19T11:59:15.681912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(1):\n#     sample = df_examples_3shot['prompts'].tolist()[i]\n#     display_prompt(sample)\n#     print('\\n')\n\n# # Check prompts\n# def colorize_test_text_fewshot(text):\n#     for word, color in zip([\"Context\",\"Example1\",\"Example2\",\"Example3\",\"Text\",\"Answer\"], \n#                            [\"blue\",\"orange\",\"orange\",\"orange\",\"red\",\"green\"]):\n#         text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n#     return text\n\n# def display_test_prompt_fewshot(sample):\n#     sample = colorize_test_text_fewshot(sample)\n#     display(Markdown(sample))\n\n# for i in range(1):\n#     sample = test_data['prompt_3shot'].tolist()[i]\n#     display_test_prompt_fewshot(sample)\n#     print('\\n')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.5. Prompt for Five-Shot Learning</b></span>\n\n<span style='color: green;'><b>Five-Shot Learning: Providing context to define specific tasks, and five examples (include Text + Answer).</b></span>\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the fiveshot_prompt</b></span>: \n<blockquote>\n    Context + Example1(Text+Answer) + Example2(Text+Answer) + Example3(Text+Answer) + Example4(Text+Answer) + Example5(Text+Answer) + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Gather 5 exampleÔºö\n# You can choose the row indexe as you like\n# Here I have chosen row indexe=1319 which contains 0 label and has the fewest tokens.\n#                    row indexe=2049 which contains 4 labels and has the fewest tokens.\n#                    row indexe=2501,2593,2680 which contain 7 labels and fewest tokens.\n\nrow_5shot = [1319,2049,2501,2593,2680] \ndf_examples_5shot = train_data.loc[row_5shot]\n\n# Generate example's prompt\ntrain_template_prompt = \"\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\n# Generate example's prompts\ndf_examples_5shot['prompts'] = df_examples_5shot.apply(lambda row: train_template_prompt.format(\n                                                                    Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]), \n                                                                    Answer=row[\"answers\"]), axis=1)\n\n# Generate five-shot test's prompt\nprompt_5shot = \"\\n\\nContext:\\n{Context}\\n\\nExample1:\\n{Example1}\\n\\nExample2:\\n{Example2}\\n\\nExample3:\\n{Example3}\\n\\nExample4:\\n{Example4}\\n\\nExample5:\\n{Example5}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\ncontext = f\"\"\"Please identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), \nphone number(PHONE_NUM), street address(STREET_ADDRESS), personal url(URL_PERSONAL) and user name(USERNAME). List each category and its entities in json format.\"\"\"\n\nexample_prompts_5shot = df_examples_5shot['prompts'].tolist()\n\n# Generate test example's prompts\ntest_data['prompt_5shot'] = test_data.apply(lambda row: prompt_5shot.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Example1=example_prompts_5shot[0], \n                                                                            Example2=example_prompts_5shot[1], \n                                                                            Example3=example_prompts_5shot[2],\n                                                                            Example4=example_prompts_5shot[3],\n                                                                            Example5=example_prompts_5shot[4],\n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.710889Z","iopub.execute_input":"2024-03-19T11:59:15.711455Z","iopub.status.idle":"2024-03-19T11:59:15.730795Z","shell.execute_reply.started":"2024-03-19T11:59:15.711406Z","shell.execute_reply":"2024-03-19T11:59:15.729252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(1):\n#     sample = df_examples_5shot['prompts'].tolist()[i]\n#     display_prompt(sample)\n#     print('\\n')\n\n# # Check prompts\n# def colorize_test_text_5shot(text):\n#     for word, color in zip([\"Context\",\"Example1\",\"Example2\",\"Example3\",\"Example4\",\"Example5\",\"Text\",\"Answer\"], \n#                            [\"blue\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"red\",\"green\"]):\n#         text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n#     return text\n\n# def display_test_prompt_5shot(sample):\n#     # Give colors to Text and Answer\n#     sample = colorize_test_text_5shot(sample)\n#     # Show sample in markdown\n#     display(Markdown(sample))\n\n# for i in range(1):\n#     sample = test_data['prompt_5shot'].tolist()[i]\n#     display_test_prompt_5shot(sample)\n#     print('\\n')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.6. Prompt for Seven-Shot Learning</b></span>\n\n<span style='color: green;'><b>Seven-Shot Learning: Providing context to define specific tasks, and seven examples (include Text + Answer).</b></span>\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the sevenshot_prompt</b></span>: \n<blockquote>\n    Context + Example1(Text+Answer) + Example2(Text+Answer) + Example3(Text+Answer) + Example4(Text+Answer) + Example5(Text+Answer) + Example6(Text+Answer) + Example7(Text+Answer) + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Gather 7 exampleÔºö\n# You can choose the row indexe as you like\n# Here I have chosen row indexe=1319 which contains 0 label and has the fewest tokens.\n#                    row indexe=2049,1556 which contain 4 labels and has the fewest tokens.\n#                    row indexe=2501,2593,2680,2051 which contain 7 labels and fewest tokens.\n\nrow_7shot = [1319,2049,1556,2501,2593,2680,2051] \ndf_examples_7shot = train_data.loc[row_7shot]\n\n# Generate example's prompt\ntrain_template_prompt = \"\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\n# Generate example's prompts\ndf_examples_7shot['prompts'] = df_examples_7shot.apply(lambda row: train_template_prompt.format(\n                                                                    Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]), \n                                                                    Answer=row[\"answers\"]), axis=1)\n\n# Generate seven-shot test's prompt\nprompt_7shot = \"\\n\\nContext:\\n{Context}\\n\\nExample1:\\n{Example1}\\n\\nExample2:\\n{Example2}\\n\\nExample3:\\n{Example3}\\n\\nExample4:\\n{Example4}\\n\\nExample5:\\n{Example5}\\n\\nExample6:\\n{Example6}\\n\\nExample7:\\n{Example7}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\ncontext = f\"\"\"Please identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), \nphone number(PHONE_NUM), street address(STREET_ADDRESS), personal url(URL_PERSONAL) and user name(USERNAME). List each category and its entities in json format.\"\"\"\n\nexample_prompts_7shot = df_examples_7shot['prompts'].tolist()\n\n# Generate test example's prompts\ntest_data['prompt_7shot'] = test_data.apply(lambda row: prompt_7shot.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Example1=example_prompts_7shot[0], \n                                                                            Example2=example_prompts_7shot[1], \n                                                                            Example3=example_prompts_7shot[2],\n                                                                            Example4=example_prompts_7shot[3],\n                                                                            Example5=example_prompts_7shot[4],\n                                                                            Example6=example_prompts_7shot[5],\n                                                                            Example7=example_prompts_7shot[6],\n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.758805Z","iopub.execute_input":"2024-03-19T11:59:15.759665Z","iopub.status.idle":"2024-03-19T11:59:15.772811Z","shell.execute_reply.started":"2024-03-19T11:59:15.759618Z","shell.execute_reply":"2024-03-19T11:59:15.771695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(1):\n#     sample = df_examples_7shot['prompts'].tolist()[i]\n#     display_prompt(sample)\n#     print('\\n')\n\n# # Check prompts\n# def colorize_test_text_7shot(text):\n#     for word, color in zip([\"Context\",\"Example1\",\"Example2\",\"Example3\",\"Example4\",\"Example5\",\"Example6\",\"Example7\",\"Text\",\"Answer\"], \n#                            [\"blue\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"red\",\"green\"]):\n#         text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n#     return text\n\n# def display_test_prompt_7shot(sample):\n#     # Give colors to Text and Answer\n#     sample = colorize_test_text_7shot(sample)\n#     # Show sample in markdown\n#     display(Markdown(sample))\n\n# for i in range(1):\n#     sample = test_data['prompt_7shot'].tolist()[i]\n#     display_test_prompt_7shot(sample)\n#     print('\\n')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  ü™Ñ<span style='color: Orange;'><b>2.7. Prompt for Nine-Shot Learning</b></span>\n\n<span style='color: green;'><b>Nine-Shot Learning: Providing context to define specific tasks, and nine examples (include Text + Answer).</b></span>\n\nDefine the <span style='color: brown;'><b>Context</b></span>:\n<blockquote>\n\nPlease identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), phone number(PHONE_NUM), street address(STREET_ADDRESS), personal URL(URL_PERSONAL), and username(USERNAME).\n\nList each category and its entities in JSON format: {'EMAIL': [], 'ID_NUM': [], 'NAME_STUDENT': [], 'PHONE_NUM': [], 'STREET_ADDRESS': [], 'URL_PERSONAL': [], 'USERNAME': []}\n\n</blockquote>\n    \nDefine the <span style='color: brown;'><b>template of the sevenshot_prompt</b></span>: \n<blockquote>\n    Context + Example1(Text+Answer) + Example2(Text+Answer) + Example3(Text+Answer) + Example4(Text+Answer) + Example5(Text+Answer) + Example6(Text+Answer) + Example7(Text+Answer) + Example8(Text+Answer) + Example9(Text+Answer) + Text + Answer.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# Gather 9 exampleÔºö\n# You can choose the row indexe as you like\n# Here I have chosen row indexe=1319,1340 which contain 0 label and has the fewest tokens.\n#                    row indexe=2049,1556 which contain 4 labels and has the fewest tokens.\n#                    row indexe=2501,2593,2680,2051,2765 which contain 7 labels and fewest tokens.\n\nrow_9shot = [1319,1340,2049,1556,2501,2593,2680,2051,2765] \ndf_examples_9shot = train_data.loc[row_9shot]\n\n# Generate example's prompt\ntrain_template_prompt = \"\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\n# Generate example's prompts\ndf_examples_9shot['prompts'] = df_examples_9shot.apply(lambda row: train_template_prompt.format(\n                                                                    Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]), \n                                                                    Answer=row[\"answers\"]), axis=1)\n\n# Generate few-shot test's prompt\nprompt_9shot = \"\\n\\nContext:\\n{Context}\\n\\nExample1:\\n{Example1}\\n\\nExample2:\\n{Example2}\\n\\nExample3:\\n{Example3}\\n\\nExample4:\\n{Example4}\\n\\nExample5:\\n{Example5}\\n\\nExample6:\\n{Example6}\\n\\nExample7:\\n{Example7}\\n\\nExample8:\\n{Example8}\\n\\nExample9:\\n{Example9}\\n\\nText:\\n{Text}\\n\\nAnswer:\\n{Answer}\"\n\ncontext = f\"\"\"Please identify the named entities in the text below, including student's name(NAME_STUDENT), ID(ID_NUM), email address(EMAIL), \nphone number(PHONE_NUM), street address(STREET_ADDRESS), personal url(URL_PERSONAL) and user name(USERNAME). List each category and its entities in json format.\"\"\"\n\nexample_prompts_9shot = df_examples_9shot['prompts'].tolist()\n\n\n# Generate test example's prompts\ntest_data['prompt_9shot'] = test_data.apply(lambda row: prompt_9shot.format(\n                                                                            Context=context_zeroshot.strip(), \n                                                                            Example1=example_prompts_9shot[0], \n                                                                            Example2=example_prompts_9shot[1], \n                                                                            Example3=example_prompts_9shot[2],\n                                                                            Example4=example_prompts_9shot[3],\n                                                                            Example5=example_prompts_9shot[4],\n                                                                            Example6=example_prompts_9shot[5],\n                                                                            Example7=example_prompts_9shot[6],\n                                                                            Example8=example_prompts_9shot[7],\n                                                                            Example9=example_prompts_9shot[8],\n                                                                            Text=' '.join(row[\"tokens\"]), #[:train_token_length_in_prompt]\n                                                                            Answer=\"\"), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:59:15.815393Z","iopub.execute_input":"2024-03-19T11:59:15.816082Z","iopub.status.idle":"2024-03-19T11:59:15.830719Z","shell.execute_reply.started":"2024-03-19T11:59:15.816046Z","shell.execute_reply":"2024-03-19T11:59:15.828328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(1):\n#     sample = df_examples_9shot['prompts'].tolist()[i]\n#     display_prompt(sample)\n#     print('\\n')\n\n# # Check prompts\n# def colorize_test_text_9shot(text):\n#     for word, color in zip([\"Context\",\"Example1\",\"Example2\",\"Example3\",\"Example4\",\"Example5\",\"Example6\",\"Example7\",\"Example8\",\"Example9\",\"Text\",\"Answer\"], \n#                            [\"blue\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"orange\",\"red\",\"green\"]):\n#         text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n<span style='color: {color};'><b>{word}:</b></span>\")\n#     return text\n\n# def display_test_prompt_9shot(sample):\n#     sample = colorize_test_text_9shot(sample)\n#     display(Markdown(sample))\n\n# for i in range(1):\n#     sample = test_data['prompt_9shot'].tolist()[i]\n#     display_test_prompt_9shot(sample)\n#     print('\\n')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü§ñ<span style='color: Orange;'><b>3. Load Gemma_2b_en Model</b></span>\n\n\nOn Kaggle, [Gemma series](https://www.kaggle.com/models/keras/gemma) models are readily available. \n\nYou can choose between <span style='color: brown;'><b>Gemma 2b</b></span> or <span style='color: brown;'><b>Gemma 7b</b></span> for your usage.\n\nThe specified sampling strategy while text generation is  <span style='color: brown;'><b>TopKSampler</b></span>, which selects the top k candidates with the highest probabilities from each position's candidate words as output. This ensures that the generated text is more diverse and of higher quality.","metadata":{}},{"cell_type":"code","source":"# You can active 'mixed_bfloat16' to reduce the learning time, it will also reduce the performance.\n# keras.mixed_precision.set_global_policy('mixed_bfloat16')\n\ngemma_tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_name)\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_name)\ngemma_lm.summary()\n\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=seed)\ngemma_lm.compile(sampler=sampler)\n\ngemma_lm.dtype_policy","metadata":{"execution":{"iopub.status.busy":"2024-03-18T00:27:04.146629Z","iopub.execute_input":"2024-03-18T00:27:04.146894Z","iopub.status.idle":"2024-03-18T00:28:03.715348Z","shell.execute_reply.started":"2024-03-18T00:27:04.146872Z","shell.execute_reply":"2024-03-18T00:28:03.714406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìñ<span style='color: Orange;'><b>4. N shot Learning</b></span>\n\nDuring the learning period, I will record the learning time for each paradigm in the <span style='color: brown;'><b>all_test_time</b></span> variable.\n\n###  üìñ<span style='color: Orange;'><b>4.1 Zero Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"all_test_time = []","metadata":{"execution":{"iopub.status.busy":"2024-03-18T00:28:03.716596Z","iopub.execute_input":"2024-03-18T00:28:03.717211Z","iopub.status.idle":"2024-03-18T00:28:03.721327Z","shell.execute_reply.started":"2024-03-18T00:28:03.717177Z","shell.execute_reply":"2024-03-18T00:28:03.720283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time_zeroshot = time.time()\n\nzeroshot_outs = []\nfor prompt in tqdm(test_data[\"zeroshot_prompts\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_zeroshot)\n    zeroshot_outs.append(out)\n    \nend_time_zeroshot = time.time() \nelapsed_time_zeroshot = end_time_zeroshot - start_time_zeroshot\nall_test_time.append(elapsed_time_zeroshot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_zeroshot} seconds, {round(elapsed_time_zeroshot/60, 2)} min, , {round(elapsed_time_zeroshot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T00:29:03.414019Z","iopub.execute_input":"2024-03-18T00:29:03.414950Z","iopub.status.idle":"2024-03-18T00:34:49.759234Z","shell.execute_reply.started":"2024-03-18T00:29:03.414909Z","shell.execute_reply":"2024-03-18T00:34:49.758126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  üìñ<span style='color: Orange;'><b>4.2 One Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"start_time_oneshot = time.time()\n\noneshot_outs = []\nfor prompt in tqdm(test_data[\"oneshot_prompts\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_oneshot)\n    oneshot_outs.append(out)\n    \nend_time_oneshot = time.time()\nelapsed_time_oneshot = end_time_oneshot - start_time_oneshot\nall_test_time.append(elapsed_time_oneshot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_oneshot} seconds, {round(elapsed_time_oneshot/60, 2)} min, , {round(elapsed_time_oneshot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T00:34:49.760884Z","iopub.execute_input":"2024-03-18T00:34:49.761214Z","iopub.status.idle":"2024-03-18T00:53:19.208719Z","shell.execute_reply.started":"2024-03-18T00:34:49.761186Z","shell.execute_reply":"2024-03-18T00:53:19.207799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  üìñ<span style='color: Orange;'><b>4.3 Three Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"start_time_3shot = time.time()\n\nthreeshot_outs = []\nfor prompt in tqdm(test_data[\"prompt_3shot\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_3shot)\n    threeshot_outs.append(out)\n    \nend_time_3shot = time.time()\nelapsed_time_3shot = end_time_3shot - start_time_3shot\nall_test_time.append(elapsed_time_3shot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_3shot} seconds, {round(elapsed_time_3shot/60, 2)} min, , {round(elapsed_time_3shot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T00:53:19.209992Z","iopub.execute_input":"2024-03-18T00:53:19.210345Z","iopub.status.idle":"2024-03-18T01:30:09.101033Z","shell.execute_reply.started":"2024-03-18T00:53:19.210318Z","shell.execute_reply":"2024-03-18T01:30:09.100106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  üìñ<span style='color: Orange;'><b>4.4 Five Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"start_time_5shot = time.time()\n\nfiveshot_outs = []\nfor prompt in tqdm(test_data[\"prompt_5shot\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_5shot)\n    fiveshot_outs.append(out)\n    \nend_time_5shot = time.time()\nelapsed_time_5shot = end_time_5shot - start_time_5shot\nall_test_time.append(elapsed_time_5shot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_5shot} seconds, {round(elapsed_time_5shot/60, 2)} min, , {round(elapsed_time_5shot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T01:30:09.103162Z","iopub.execute_input":"2024-03-18T01:30:09.103455Z","iopub.status.idle":"2024-03-18T02:04:50.014775Z","shell.execute_reply.started":"2024-03-18T01:30:09.103430Z","shell.execute_reply":"2024-03-18T02:04:50.013821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  üìñ<span style='color: Orange;'><b>4.5 Seven Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"start_time_7shot = time.time()\n\nsevenshot_outs = []\nfor prompt in tqdm(test_data[\"prompt_7shot\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_7shot)\n    sevenshot_outs.append(out)\n    \nend_time_7shot = time.time()\nelapsed_time_7shot = end_time_7shot - start_time_7shot\nall_test_time.append(elapsed_time_7shot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_7shot} seconds, {round(elapsed_time_7shot/60, 2)} min, , {round(elapsed_time_7shot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T02:04:50.016120Z","iopub.execute_input":"2024-03-18T02:04:50.016407Z","iopub.status.idle":"2024-03-18T02:42:59.540319Z","shell.execute_reply.started":"2024-03-18T02:04:50.016382Z","shell.execute_reply":"2024-03-18T02:42:59.539344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  üìñ<span style='color: Orange;'><b>4.6 Nine Shot Learning</b></span>","metadata":{}},{"cell_type":"code","source":"start_time_9shot = time.time()\n\nnineshot_outs = []\nfor prompt in tqdm(test_data[\"prompt_9shot\"]):\n    out = gemma_lm.generate(prompt, max_length=max_length_pred_9shot)\n    nineshot_outs.append(out)\n    \nend_time_9shot = time.time()\nelapsed_time_9shot = end_time_9shot - start_time_9shot\nall_test_time.append(elapsed_time_9shot)\n\nprint(f\"Execution TimeÔºö\\n{elapsed_time_9shot} seconds, {round(elapsed_time_9shot/60, 2)} min, , {round(elapsed_time_9shot/3600, 2)} h\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T02:42:59.541562Z","iopub.execute_input":"2024-03-18T02:42:59.541861Z","iopub.status.idle":"2024-03-18T03:14:28.884347Z","shell.execute_reply.started":"2024-03-18T02:42:59.541836Z","shell.execute_reply":"2024-03-18T03:14:28.883374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üåü<span style='color: Orange;'><b> 5. Post-processing of Predictions</b></span>\n\nIn this section, I have referenced some of the function definitions.\n\n<span style='color: green;'><b>The list of references of Notebook is as follows:</b></span>\n<blockquote>\n    \n[piidd-let-s-go-higher](https://www.kaggle.com/code/hyunsoolee1010/piidd-let-s-go-higher)\n    \n[rule-based-approach](https://www.kaggle.com/code/emiz6413/rule-based-approach)\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"####  ‚ö†Ô∏è<span style='color: Orange;'><b> You should run this block if you passed the learning phase.</b></span>","metadata":{}},{"cell_type":"code","source":"# To get the outputs of different learning paradigms.\n\n# my_dicts = json.load(open(\"/kaggle/input/temp-test-gemma-output/my_dict4.json\"))\n# zeroshot_outs = my_dicts['zeroshot_outs']\n# oneshot_outs = my_dicts['oneshot_outs']\n# threeshot_outs = my_dicts['threeshot_outs']\n# fiveshot_outs = my_dicts['fiveshot_outs']\n# sevenshot_outs = my_dicts['sevenshot_outs']\n# nineshot_outs = my_dicts['nineshot_outs']\n\n# zeroshot_time = my_dicts[\"zeroshot_time\"]\n# oneshot_time = my_dicts[\"oneshot_time\"]\n# threeshot_time = my_dicts[\"threeshot_time\"]\n# fiveshot_time = my_dicts[\"fiveshot_time\"]\n# sevenshot_time = my_dicts[\"sevenshot_time\"]\n# nineshot_time = my_dicts[\"nineshot_time\"]\n\n# all_test_time = [zeroshot_time,oneshot_time,threeshot_time,fiveshot_time,sevenshot_time, nineshot_time]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T12:10:50.515332Z","iopub.execute_input":"2024-03-19T12:10:50.515886Z","iopub.status.idle":"2024-03-19T12:10:50.523024Z","shell.execute_reply.started":"2024-03-19T12:10:50.515839Z","shell.execute_reply":"2024-03-19T12:10:50.521299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Used as tokenizer when find_span()\nnlp = English()\n\n# This function is used to match all the indices in the original list of tokens (document)\n# where the token of the answer (target) is located.\n# refÔºöhttps://www.kaggle.com/code/emiz6413/rule-based-approach\ndef find_span(target, document):\n    # target: the token list of the matched phone number (after tokenizer processing).\n    # document: the token list of all text.\n    \n    # idx is used for character counting\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        # print(f\"token id:{i}, token str: {token} \")\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            if span not in spans:\n                spans.append(span)\n                span = []\n                idx = 0\n                continue\n            else:\n                span = []\n                idx = 0\n    return spans\n\ndef get_answers_from_prediction(pred, type_train):\n    # This function is used to extract the answer part from the raw output of the model\n    n = int(type_train.split(\"-\")[0]) # get n for \"n-shot\"\n    if n != 0: # not zero-shot\n        split_pred = pred.split(f\"Example{n}:\\n\\n\")[1]\n        all_res = split_pred.split(\"Answer:\\n\")[2].strip()\n    else: # zero-shot\n        all_res = pred.split(\"Answer:\\n\")[1].strip()\n\n    try:\n        all_res = all_res.split(\"\\nText:\")[0].strip()\n    except:\n        pass\n    \n    # Post-process of answers\n    # Change '' to \"\", remove \\n, \\t, \" \", \n    # Add ]} if not in answer, cuz we assume the answer should in json format, and want it is closed\n    text = str(all_res.replace(\"'\", '\"'))\n    text = str(text.replace(\"\\n\", ''))\n    text = str(text.replace(\"\\t\", ''))\n    text = str(text.replace(\" \", ''))\n    if \"]}\" not in text: text+= \"]}\"\n\n    return text\n\ndef porcess_matched_list(match):\n    # m is a list\n    matched = [m for m in match if m!=\"\"]\n    list_match = []\n    for ele in matched:\n        # ‚Äúemail@com‚Äù, \"eamil2@com\" -> '', 'email@com', ', ','email2@com', ''\n        ele_splitted = ele.split('\"')\n        # print(\"ele_splitted\", ele_splitted)\n        ele_splitted2 = []\n        for element in ele_splitted:\n            # email@com,eamil2@com -> 'email@com', 'email2@com'\n            ele_splitted2 += element.split(\",\")\n        # print(\"ele_splitted2\", ele_splitted2)\n        \n        # clean incorrect elements/text in the final answer\n        ans = [element.strip() for element in ele_splitted2 if element not in [',', '', ' ',', ', 'VALUE', 'value', '{', '}']]\n        list_match+=ans\n    return list_match\n\n# This function is used to extract the answer from the raw output of the model and convert to dict\ndef postprocess_output_to_dict(y_preds_raw, type_train):\n    # Register answers of all examples in res_dicts\n    res_dicts = []\n    for i, pred in enumerate(y_preds_raw):\n        text = get_answers_from_prediction(pred, type_train)\n        # Register answers of one example in res_dict\n        res_dict = {}\n        for l in test_labels[:-1]:\n            # Define as much as possible the rules to match answers generated by Gemma\n            pattern = f\"\\\"*{l}\\\"*:\\[(.*?)\\]\" #\"NAME_STUDENT\":[...]\n            pattern2 = f\"\\\"*{l}\\\"*,\\[(.*?)\\]\" #\"NAME_STUDENT\",[...]\n            pattern3 = f'\\\"*{l}\\\"*:\"(.*?)\"'#\"NAME_STUDENT\":\"...\"\n            match = re.findall(pattern, text)\n            match2 = re.findall(pattern2, text)\n            match3 = re.findall(pattern3, text)\n\n            is_matched = False\n            if match != []:\n                list_match = porcess_matched_list(match)\n                res_dict[l] = list_match \n                is_matched = True\n            elif match2 != [] and not is_matched:\n                list_match2 = porcess_matched_list(match2)\n                res_dict[l] = list_match2\n                is_matched = True\n            elif match3 != [] and not is_matched:\n                list_match3 = porcess_matched_list(match3)\n                res_dict[l] = list_match3\n                is_matched = True\n            else:\n                res_dict[l] = []\n        res_dicts.append(res_dict)\n    return res_dicts\n\n# This function is used to prepare the new dictionary so that it contains the info from the original \n# example and get the tokens-strÔºåtoken-index corresponding and BIO label to compute the f5 score.\n# ref: https://www.kaggle.com/code/hyunsoolee1010/piidd-let-s-go-higher\ndef generate_output_df(res_dicts, df_test):\n    real_ypreds = []\n    for i, rd in enumerate(res_dicts):\n        # If existe valid values in dict \n        if list(rd.values()) != [[] for i in range(len(test_labels[:-1]))]:\n            _data = df_test.loc[i]\n            for k in rd.keys():\n                values = rd[k]\n                for v in values:\n                    if v not in [',', '', ' ',', ', 'VALUE', 'value', '{', '}']:\n                        target = [t.text for t in nlp.tokenizer(v)] \n                        matched_spans = find_span(target, _data[\"tokens\"])\n                        # print(\"target: \", target)\n                        # print(\"_data: \", _data[\"tokens\"])\n                        # print(\"matched_spans: \", matched_spans)\n                        for matched_span in matched_spans:\n                            for intermediate, token_idx in enumerate(matched_span):\n                                prefix = \"I\" if intermediate else \"B\"\n                                elem = {\"document\": _data[\"document\"], \n                                       \"token\": token_idx, \n                                       \"label\": f\"{prefix}-{k}\", \n                                       \"token_str\": _data[\"tokens\"][token_idx]}\n                                if elem not in real_ypreds:\n                                    real_ypreds.append(elem)\n    return real_ypreds\n\n# refÔºöhttps://www.kaggle.com/code/emiz6413/rule-based-approach\n# I use the f5 score as it is used by the competition. \ndef pii_fbeta_score(pred_df, gt_df, beta=5):\n    \"\"\"\n    Parameters:\n    - pred_df (DataFrame): DataFrame containing predicted PII labels.\n    - gt_df (DataFrame): DataFrame containing ground truth PII labels.\n    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n\n    Returns:\n    - float: Micro F-beta score.\n    \"\"\"   \n    df = pred_df.merge(gt_df,how='outer',on=['document',\"token\"],suffixes=('_pred','_gt'))\n\n    df['cm'] = \"\"\n\n    df.loc[df.label_gt.isna(),'cm'] = \"FP\"\n\n\n    df.loc[df.label_pred.isna(),'cm'] = \"FN\"\n    df.loc[(df.label_gt.notna()) & (df.label_gt!=df.label_pred),'cm'] = \"FN\"\n\n    df.loc[(df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt==df.label_pred),'cm'] = \"TP\"\n    \n    FP = (df['cm']==\"FP\").sum()\n    FN = (df['cm']==\"FN\").sum()\n    TP = (df['cm']==\"TP\").sum()\n\n    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n\n    return s_micro","metadata":{"execution":{"iopub.status.busy":"2024-03-19T12:09:16.789039Z","iopub.execute_input":"2024-03-19T12:09:16.789504Z","iopub.status.idle":"2024-03-19T12:09:17.174465Z","shell.execute_reply.started":"2024-03-19T12:09:16.789470Z","shell.execute_reply":"2024-03-19T12:09:17.173222Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the ground truth.\n# refÔºöhttps://www.kaggle.com/code/emiz6413/rule-based-approach\ngt = []\nfor _, row in test_data.iterrows():\n    for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n        if label == \"O\":\n            continue\n        gt.append(\n            {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label}\n        )\ngt_df = pd.DataFrame(gt)\ngt_df[\"row_id\"] = gt_df.index\ngt_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T12:09:22.696060Z","iopub.execute_input":"2024-03-19T12:09:22.696576Z","iopub.status.idle":"2024-03-19T12:09:22.777378Z","shell.execute_reply.started":"2024-03-19T12:09:22.696536Z","shell.execute_reply":"2024-03-19T12:09:22.776187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the score.\nf5s = []\ntypes = [\"0-shot\",\"1-shot\", \"3-shot\", \"5-shot\", \"7-shot\", \"9-shot\"]  # \nfor i, pred in enumerate([zeroshot_outs, oneshot_outs, threeshot_outs, fiveshot_outs, sevenshot_outs, nineshot_outs]):\n    res_dicts = postprocess_output_to_dict(pred, types[i])\n    real_ypreds = generate_output_df(res_dicts, test_data)\n    \n    pred_df = pd.DataFrame(real_ypreds)\n    pred_df = pred_df.sort_values(by=['token'])\n    pred_df = pred_df.sort_values(by=['document'])\n    pred_df = pred_df.reset_index(drop=True)\n    pred_df[\"row_id\"] = pred_df.index\n\n    f5 = pii_fbeta_score(pred_df, gt_df, beta=5)\n    print(f\"F5 score of {types[i]}: {f5}\")\n    f5s.append(f5)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T12:09:28.905745Z","iopub.execute_input":"2024-03-19T12:09:28.906266Z","iopub.status.idle":"2024-03-19T12:09:31.696673Z","shell.execute_reply.started":"2024-03-19T12:09:28.906228Z","shell.execute_reply":"2024-03-19T12:09:31.695219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the outputs.\nmy_dict = {'zeroshot_time': all_test_time[0], \n           'oneshot_time': all_test_time[1], \n           'threeshot_time': all_test_time[2],\n           'fiveshot_time': all_test_time[3],\n           'sevenshot_time': all_test_time[4],\n           'nineshot_time': all_test_time[5],\n\n           'zeroshot_f5': f5s[0],\n           'oneshot_f5': f5s[1],\n           'threeshot_f5': f5s[2],\n           'fiveshot_f5': f5s[3],\n           'sevenshot_f5': f5s[4],\n           'nineshot_f5': f5s[5],\n           \n           \"zeroshot_outs\":zeroshot_outs,\n           \"oneshot_outs\":oneshot_outs,\n           \"threeshot_outs\":threeshot_outs,\n           \"fiveshot_outs\":fiveshot_outs,\n           \"sevenshot_outs\":sevenshot_outs,\n           \"nineshot_outs\":nineshot_outs\n           }\n\n# Save dict\nwith open('/kaggle/working/my_dict.json', 'w') as f:\n    json.dump(my_dict, f)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T03:18:56.120160Z","iopub.execute_input":"2024-03-18T03:18:56.120794Z","iopub.status.idle":"2024-03-18T03:18:56.169387Z","shell.execute_reply.started":"2024-03-18T03:18:56.120763Z","shell.execute_reply":"2024-03-18T03:18:56.168502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all f5 scores and learning time\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfont_family = \"Arial\"\nfig = make_subplots(rows=2, cols=1) #, subplot_titles=(\"TrainSet\", \"TestSet\")\n\n# Define data to plot\nx1 = types\ny1 = f5s\n\nx2 = types\ny2 = all_test_time\n\n# Add the first line chart to the first column\nfig.add_trace(go.Scatter(x=x1, y=y1, mode='lines', name='F5-score'), 1, 1)\nfig.add_trace(go.Scatter(x=x2, y=y2, mode='lines', name='Training-Time'), 2, 1)\n\n# Update Layout Settings\nfig.update_layout(width=800, height=800,paper_bgcolor=\"WHITE\")\n\nfig.update_layout(title_text='Performance on N-Shot Learning Paradigms',\n                  font=dict(family=font_family,\n                            size=10,\n                            color=\"#75767A\"),\n                 )\nfig.update_yaxes(title_text=\"F5 Score\", row=1, col=1)\nfig.update_yaxes(title_text=\"Learning Time (s)\", row=2, col=1)\n\nfig.update_layout(title_font_family=font_family,\n                  title_font_color='BLACK',\n                  title_font_size=15,\n                  title_x=0.5)\n\nfig.update_layout(legend_title=\"\",\n                  showlegend=True,\n                  legend=dict(x=0.33, y=1.06, orientation=\"h\")\n                 )\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T12:10:54.281455Z","iopub.execute_input":"2024-03-19T12:10:54.282374Z","iopub.status.idle":"2024-03-19T12:10:54.389182Z","shell.execute_reply.started":"2024-03-19T12:10:54.282327Z","shell.execute_reply":"2024-03-19T12:10:54.387916Z"},"trusted":true},"execution_count":null,"outputs":[]}]}